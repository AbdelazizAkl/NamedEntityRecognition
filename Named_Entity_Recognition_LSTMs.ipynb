{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbdelazizAkl/NamedEntityRecognition/blob/main/Named_Entity_Recognition_LSTMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6czvz5VKO5M"
      },
      "source": [
        "# Notebook for Programming in Problem 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5o8HI5JqTvU5"
      },
      "source": [
        "## Learning Objectives\n",
        "In this problem, we will use [PyTorch](https://pytorch.org/) to implement long short-term memory (LSTM) for named entity recognition (NER). We will use the same dataset and boilerplate code as in Programming Problem 1 of Assignment #3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObrHyvWvTyGZ"
      },
      "source": [
        "## Writing Code\n",
        "Look for the keyword \"TODO\" and fill in your code in the empty space.\n",
        "Feel free to change function signatures, but be careful that you might need to also change how they are called in other parts of the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6YTnpgbFdMI",
        "outputId": "23a86758-e067-458c-db88-d6f8850ba0f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon May  6 16:33:09 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   68C    P8              11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi # you may need to try reconnecting to get a T4 gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnYMKJlKNXYe"
      },
      "source": [
        "## Installing PyTorch and Other Packages\n",
        "\n",
        "Install PyTorch using pip. See [https://pytorch.org/](https://pytorch.org/) if you want to install it on your computer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dRVuiP_JVdT",
        "outputId": "32641e1f-b9d9-432c-ce6a-2da2eeea7e53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.31.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.25.2)\n",
            "Requirement already satisfied: torchdata==0.7.1 in /usr/local/lib/python3.10/dist-packages (from torchtext) (0.7.1)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.1->torchtext) (2.0.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchtext -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPsFH637OpLy"
      },
      "source": [
        "Test if our installation works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c62StNb2NvKk",
        "outputId": "bd4a2c33-23ce-4f9d-fcd7-1642d7bfb196",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch successfully installed!\n",
            "Version: 2.2.1+cu121\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Multiply two matrices on GPU\n",
        "a = torch.rand(100, 200).cuda()\n",
        "b = torch.rand(200, 100).cuda()\n",
        "c = torch.matmul(a, b)\n",
        "\n",
        "print(\"PyTorch successfully installed!\")\n",
        "print(\"Version:\", torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qaC8sxcqkGX"
      },
      "source": [
        "Also install [scikit-learn](https://scikit-learn.org/stable/). We will use it for calculating evaluation metrics such as accuracy and F1 score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5Y2xB_uqqM9",
        "outputId": "9c368620-3d87-448c-c014-87b03c25c608",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "Successfully installed scikit-learn-1.4.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -U scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhV4CYivRbt4"
      },
      "source": [
        "Let's import all the packages at once:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjRM4cCFRh-d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.vocab import Vocab, vocab\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "import re\n",
        "from collections import Counter\n",
        "from typing import List, Tuple, Dict, Optional, Any"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yn1bIPjAN-9V"
      },
      "source": [
        "## Long Short Term Memory (LSTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJOKIneRTrTH"
      },
      "source": [
        "### Data Loading\n",
        "\n",
        "We will use the same dataset for named entity recognition in Assignment #2. First download the data and take a look at the first 50 lines:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWqz7kDxSqeb",
        "outputId": "45ecfd3c-f08b-4000-c040-074882ad41fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EU NNP I-NP ORG\n",
            "rejects VBZ I-VP O\n",
            "German JJ I-NP MISC\n",
            "call NN I-NP O\n",
            "to TO I-VP O\n",
            "boycott VB I-VP O\n",
            "British JJ I-NP MISC\n",
            "lamb NN I-NP O\n",
            ". . O O\n",
            "\n",
            "Peter NNP I-NP PER\n",
            "Blackburn NNP I-NP PER\n",
            "\n",
            "BRUSSELS NNP I-NP LOC\n",
            "1996-08-22 CD I-NP O\n",
            "\n",
            "The DT I-NP O\n",
            "European NNP I-NP ORG\n",
            "Commission NNP I-NP ORG\n",
            "said VBD I-VP O\n",
            "on IN I-PP O\n",
            "Thursday NNP I-NP O\n",
            "it PRP B-NP O\n",
            "disagreed VBD I-VP O\n",
            "with IN I-PP O\n",
            "German JJ I-NP MISC\n",
            "advice NN I-NP O\n",
            "to TO I-PP O\n",
            "consumers NNS I-NP O\n",
            "to TO I-VP O\n",
            "shun VB I-VP O\n",
            "British JJ I-NP MISC\n",
            "lamb NN I-NP O\n",
            "until IN I-SBAR O\n",
            "scientists NNS I-NP O\n",
            "determine VBP I-VP O\n",
            "whether IN I-SBAR O\n",
            "mad JJ I-NP O\n",
            "cow NN I-NP O\n",
            "disease NN I-NP O\n",
            "can MD I-VP O\n",
            "be VB I-VP O\n",
            "transmitted VBN I-VP O\n",
            "to TO I-PP O\n",
            "sheep NN I-NP O\n",
            ". . O O\n",
            "\n",
            "Germany NNP I-NP LOC\n",
            "'s POS B-NP O\n",
            "representative NN I-NP O\n"
          ]
        }
      ],
      "source": [
        "!wget --quiet https://princeton-nlp.github.io/cos484/assignments/a2/eng.train\n",
        "!wget --quiet https://princeton-nlp.github.io/cos484/assignments/a2/eng.val\n",
        "!cat eng.train | head -n 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVt1a6nzWsiF"
      },
      "source": [
        "Each line corresponds to a word. Different sentences are separated by an additional line break. Take \"EU NNP I-NP ORG\" as an example. \"EU\" is a word. \"NNP\" and \"I-NP\" are tags for POS tagging and chunking, which we will ignore. \"ORG\" is the tag for NER, which is our prediction target. There are 5 possible values for the NER tag: ORG, PER, LOC, MISC, and O.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnNfOBUYJvVW"
      },
      "outputs": [],
      "source": [
        "# A sentence is a list of (word, tag) tuples.\n",
        "# For example, [(\"hello\", \"O\"), (\"world\", \"O\"), (\"!\", \"O\")]\n",
        "Sentence = List[Tuple[str, str]]\n",
        "\n",
        "\n",
        "def read_data_file(\n",
        "    datapath: str,\n",
        ") -> Tuple[List[Sentence], Dict[str, int], Dict[str, int]]:\n",
        "    \"\"\"\n",
        "    Read and preprocess input data from the file `datapath`.\n",
        "    Example:\n",
        "    ```\n",
        "        sentences, word_cnt, tag_cnt = read_data_file(\"eng.train\")\n",
        "    ```\n",
        "    Return values:\n",
        "        `sentences`: a list of sentences, including words and NER tags\n",
        "        `word_cnt`: a Counter object, the number of occurrences of each word\n",
        "        `tag_cnt`: a Counter object, the number of occurences of each NER tag\n",
        "    \"\"\"\n",
        "    sentences: List[Sentence] = []\n",
        "    word_cnt: Dict[str, int] = Counter()\n",
        "    tag_cnt: Dict[str, int] = Counter()\n",
        "\n",
        "    for sentence_txt in open(datapath).read().split(\"\\n\\n\"):\n",
        "        if \"DOCSTART\" in sentence_txt:\n",
        "            # Ignore dummy sentences at the begining of each document.\n",
        "            continue\n",
        "        # Read a new sentence\n",
        "        sentences.append([])\n",
        "        for token in sentence_txt.split(\"\\n\"):\n",
        "            w, _, _, t = token.split()\n",
        "            # Replace all digits with \"0\" to reduce out-of-vocabulary words\n",
        "            w = re.sub(\"\\d\", \"0\", w)\n",
        "            word_cnt[w] += 1\n",
        "            tag_cnt[t] += 1\n",
        "            sentences[-1].append((w, t))\n",
        "\n",
        "    return sentences, word_cnt, tag_cnt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLMGYSZ7KxzP"
      },
      "outputs": [],
      "source": [
        "# Some helper code\n",
        "def get_device() -> torch.device:\n",
        "    \"\"\"\n",
        "    Use GPU when it is available; use CPU otherwise.\n",
        "    See https://pytorch.org/docs/stable/notes/cuda.html#device-agnostic-code\n",
        "    \"\"\"\n",
        "    return torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVHAOb7iMPwC"
      },
      "outputs": [],
      "source": [
        "def eval_metrics(ground_truth: List[int], predictions: List[int]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Calculate various evaluation metrics such as accuracy and F1 score\n",
        "    Parameters:\n",
        "        `ground_truth`: the list of ground truth NER tags\n",
        "        `predictions`: the list of predicted NER tags\n",
        "    \"\"\"\n",
        "    f1_scores = f1_score(ground_truth, predictions, average=None)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(ground_truth, predictions),\n",
        "        \"f1\": f1_scores,\n",
        "        \"average f1\": np.mean(f1_scores),\n",
        "        \"confusion matrix\": confusion_matrix(ground_truth, predictions),\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s830dhbnj1L"
      },
      "source": [
        "## Long Short-term Memory (LSTM)\n",
        "\n",
        "Now we implement an one-layer LSTM for the same task and compare it to FFNNs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "to7DnWNiY5ZS"
      },
      "source": [
        "### Data Loading **(4 points)**\n",
        "\n",
        "Like before, we first implement the data loader. But unlike before, each data example is now a variable-length sentence. How can we pack multiple sentences with different lengths into the same batch? One possible solution is to pad them to the same length using a special token. The code below illustrates the idea:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5oVgqE7JaJp"
      },
      "outputs": [],
      "source": [
        "# 3 sentences with different lengths\n",
        "sentence_1 = torch.tensor([6, 1, 2])\n",
        "sentence_2 = torch.tensor([4, 2, 7, 7, 9])\n",
        "sentence_3 = torch.tensor([3, 4])\n",
        "# Form a batch by padding 0\n",
        "sentence_batch = torch.tensor([\n",
        "    [6, 1, 2, 0, 0],\n",
        "    [4, 2, 7, 7, 9],\n",
        "    [3, 4, 0, 0, 0],\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udC0SMjkKaCN"
      },
      "source": [
        "We implement the above idea in a customized batching function `form_batch`. Optionally, see [here](https://pytorch.org/docs/stable/data.html#loading-batched-and-non-batched-data) for how batching works in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sACcGN4XYMgj"
      },
      "outputs": [],
      "source": [
        "class SequenceDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Each data example is a sentence, including its words and NER tags.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, datapath: str, words_vocab: Optional[Vocab] = None, tags_vocab: Optional[Vocab] = None\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the dataset by reading from datapath.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.sentences: List[Sentence] = []\n",
        "        UNKNOWN = \"<UNKNOWN>\"\n",
        "        PAD = \"<PAD>\"  # Special token used for padding\n",
        "\n",
        "        print(\"Loading data from %s\" % datapath)\n",
        "        self.sentences, word_cnt, tag_cnt = read_data_file(datapath)\n",
        "        print(\"%d sentences loaded.\" % len(self.sentences))\n",
        "\n",
        "        if words_vocab is None:\n",
        "            words_vocab = vocab(word_cnt, specials=[PAD, UNKNOWN])\n",
        "            words_vocab.set_default_index(words_vocab[UNKNOWN])\n",
        "\n",
        "        self.words_vocab = words_vocab\n",
        "\n",
        "        self.unknown_idx = self.words_vocab[UNKNOWN]\n",
        "        self.pad_idx = self.words_vocab[PAD]\n",
        "\n",
        "        if tags_vocab is None:\n",
        "            tags_vocab = vocab(tag_cnt, specials=[])\n",
        "        self.tags_vocab = tags_vocab\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Sentence:\n",
        "        \"\"\"\n",
        "        Get the idx'th sentence in the dataset.\n",
        "        \"\"\"\n",
        "        return self.sentences[idx]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"\n",
        "        Return the number of sentences in the dataset.\n",
        "        \"\"\"\n",
        "        # TODO: Implement this method\n",
        "        # START HERE\n",
        "        return len(self.sentences)\n",
        "        # END\n",
        "\n",
        "    def form_batch(self, sentences: List[Sentence]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        A customized function for batching a number of sentences together.\n",
        "        Different sentences have different lengths. Let max_len be the longest length.\n",
        "        When packing them into one tensor, we need to pad all sentences to max_len.\n",
        "        Return values:\n",
        "            `words`: a list in which each element itself is a list of words in a sentence\n",
        "            `word_idxs`: a batch_size x max_len tensor.\n",
        "                       word_idxs[i][j] is the index of the j'th word in the i'th sentence .\n",
        "            `tags`: a list in which each element itself is a list of tags in a sentence\n",
        "            `tag_idxs`: a batch_size x max_len tensor\n",
        "                      tag_idxs[i][j] is the index of the j'th tag in the i'th sentence.\n",
        "            `valid_mask`: a batch_size x max_len tensor\n",
        "                        valid_mask[i][j] is True if the i'th sentence has the j'th word.\n",
        "                        Otherwise, valid[i][j] is False.\n",
        "        \"\"\"\n",
        "        words: List[List[str]] = []\n",
        "        tags: List[List[str]] = []\n",
        "        max_len = -1  # length of the longest sentence\n",
        "        for sent in sentences:\n",
        "            words.append([])\n",
        "            tags.append([])\n",
        "            for w, t in sent:\n",
        "                words[-1].append(w)\n",
        "                tags[-1].append(t)\n",
        "            max_len = max(max_len, len(words[-1]))\n",
        "\n",
        "        batch_size = len(sentences)\n",
        "        word_idxs = torch.full(\n",
        "            (batch_size, max_len), fill_value=self.pad_idx, dtype=torch.int64\n",
        "        )\n",
        "        tag_idxs = torch.full_like(word_idxs, fill_value=self.tags_vocab[\"O\"])\n",
        "        valid_mask = torch.zeros_like(word_idxs, dtype=torch.bool)\n",
        "\n",
        "        ## TODO: Fill in the values in word_idxs, tag_idxs, and valid_mask\n",
        "        ## Caveat: There may be out-of-vocabulary words in validation data\n",
        "        ## See torchtext.vocab.Vocab: https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab\n",
        "        ## START HERE\n",
        "\n",
        "        for i, (sent, word_list, tag_list) in enumerate(zip(sentences, words, tags)):\n",
        "          for j, (w, t) in enumerate(zip(word_list, tag_list)):\n",
        "              word_idxs[i][j] = self.words_vocab[w]\n",
        "              tag_idxs[i][j] = self.tags_vocab[t]\n",
        "              valid_mask[i][j] = True\n",
        "\n",
        "        # END\n",
        "\n",
        "        return {\n",
        "            \"words\": words,\n",
        "            \"word_idxs\": word_idxs,\n",
        "            \"tags\": tags,\n",
        "            \"tag_idxs\": tag_idxs,\n",
        "            \"valid_mask\": valid_mask,\n",
        "        }\n",
        "\n",
        "\n",
        "def create_sequence_dataloaders(\n",
        "    batch_size: int, shuffle: bool = True\n",
        ") -> Tuple[DataLoader, DataLoader, Vocab]:\n",
        "    \"\"\"\n",
        "    Create the dataloaders for training and validaiton.\n",
        "    \"\"\"\n",
        "    ds_train = SequenceDataset(\"eng.train\")\n",
        "    ds_val = SequenceDataset(\"eng.val\", words_vocab=ds_train.words_vocab, tags_vocab=ds_train.tags_vocab)\n",
        "    loader_train = DataLoader(\n",
        "        ds_train,\n",
        "        batch_size,\n",
        "        shuffle,\n",
        "        collate_fn=ds_train.form_batch,  # customized function for batching\n",
        "        drop_last=True,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "    loader_val = DataLoader(\n",
        "        ds_val, batch_size, collate_fn=ds_val.form_batch, pin_memory=True\n",
        "    )\n",
        "    return loader_train, loader_val, ds_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2EcVxYuYvGv"
      },
      "source": [
        "Here is a simple sanity-check. Try to understand its output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TazmodGWYx2d",
        "outputId": "734c34fb-95a4-424a-e907-e5acb2357f12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from eng.train\n",
            "14041 sentences loaded.\n",
            "Loading data from eng.val\n",
            "3490 sentences loaded.\n",
            "Iterating on the training data..\n",
            "{'words': [['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], ['Peter', 'Blackburn'], ['BRUSSELS', '0000-00-00']], 'word_idxs': tensor([[ 2,  3,  4,  5,  6,  7,  8,  9, 10],\n",
            "        [11, 12,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [13, 14,  0,  0,  0,  0,  0,  0,  0]]), 'tags': [['ORG', 'O', 'MISC', 'O', 'O', 'O', 'MISC', 'O', 'O'], ['PER', 'PER'], ['LOC', 'O']], 'tag_idxs': tensor([[0, 1, 2, 1, 1, 1, 2, 1, 1],\n",
            "        [3, 3, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [4, 1, 1, 1, 1, 1, 1, 1, 1]]), 'valid_mask': tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
            "        [ True,  True, False, False, False, False, False, False, False],\n",
            "        [ True,  True, False, False, False, False, False, False, False]])}\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "def check_sequence_dataloader() -> None:\n",
        "    loader_train, _, _ = create_sequence_dataloaders(batch_size=3, shuffle=False)\n",
        "    print(\"Iterating on the training data..\")\n",
        "    for i, data_batch in enumerate(loader_train):\n",
        "        if i == 0:\n",
        "            print(data_batch)\n",
        "    print(\"Done!\")\n",
        "\n",
        "\n",
        "check_sequence_dataloader()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifk3i-obY8YB"
      },
      "source": [
        "### Implement the Model **(8 points)**\n",
        "\n",
        "Next, implement LSTM for predicting NER tags from input words. [nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM) is definitely useful. Further, it is tricky to handle sentences in the same batch with different lengths. Please read the PyTorch documentation in detail!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3V0NvQynZF8e"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    Long short-term memory for NER\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, words_vocab: Vocab, tags_vocab:Vocab, d_emb: int, d_hidden: int, bidirectional: bool) -> None:\n",
        "        \"\"\"\n",
        "        Initialize an LSTM\n",
        "        Parameters:\n",
        "            `words_vocab`: vocabulary of words\n",
        "            `tags_vocab`: vocabulary of tags\n",
        "            `d_emb`: dimension of word embeddings (D)\n",
        "            `d_hidden`: dimension of the hidden layer (H)\n",
        "            `bidirectional`: true if LSTM should be bidirectional\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # TODO: Create the word embeddings (nn.Embedding),\n",
        "        #       the LSTM (nn.LSTM) and the output layer (nn.Linear).\n",
        "        #       Read the torch docs for additional guidance : https://pytorch.org/docs/stable\n",
        "        #       Note: Pay attention to the LSTM output shapes!\n",
        "        # START HERE\n",
        "\n",
        "        # Create the word embeddings\n",
        "        self.word_embeddings = nn.Embedding(len(words_vocab), d_emb)\n",
        "\n",
        "        # Create the LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=d_emb,\n",
        "            hidden_size=d_hidden,\n",
        "            bidirectional=bidirectional,\n",
        "            batch_first=True  # Input and output tensors are provided as (batch, seq, feature)\n",
        "        )\n",
        "\n",
        "        # Create the output layer\n",
        "        self.output_layer = nn.Linear(\n",
        "            in_features=d_hidden * 2 if bidirectional else d_hidden,  # Double hidden size if bidirectional\n",
        "            out_features=len(tags_vocab)  # Output logits for each tag\n",
        "        )\n",
        "\n",
        "        # END\n",
        "\n",
        "    def forward(\n",
        "        self, word_idxs: torch.Tensor, valid_mask: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Given words in sentences, predict the logits of the NER tag.\n",
        "        Parameters:\n",
        "            `word_idxs`: a batch_size x max_len tensor\n",
        "            `valid_mask`: a batch_size x max_len tensor\n",
        "        Return values:\n",
        "            `logits`: a batch_size x max_len x 5 tensor\n",
        "        \"\"\"\n",
        "        # TODO: Implement the forward pass\n",
        "        # START HERE\n",
        "\n",
        "        # Get word embeddings\n",
        "        embedded_words = self.word_embeddings(word_idxs)  # batch_size x max_len x d_emb\n",
        "\n",
        "        # Pack the padded sequences\n",
        "        packed_embedded_words = nn.utils.rnn.pack_padded_sequence(\n",
        "            embedded_words, valid_mask.sum(dim=1).cpu(), batch_first=True, enforce_sorted=False\n",
        "        )\n",
        "\n",
        "        # Pass through the LSTM\n",
        "        packed_output, _ = self.lstm(packed_embedded_words)\n",
        "\n",
        "        # Unpack the packed sequences\n",
        "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
        "\n",
        "        # Pass through the output layer\n",
        "        logits = self.output_layer(output)  # batch_size x max_len x len(tags_vocab)\n",
        "\n",
        "        # END\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BFTKaB4Zydx"
      },
      "source": [
        "We do a sanity-check by loading a batch of data examples and pass it through the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKg1ni4QZ6D1",
        "outputId": "df45df88-6d84-4074-ced5-6cb341cec1f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from eng.train\n",
            "14041 sentences loaded.\n",
            "Loading data from eng.val\n",
            "3490 sentences loaded.\n",
            "LSTM(\n",
            "  (word_embeddings): Embedding(20102, 64)\n",
            "  (lstm): LSTM(64, 128, batch_first=True, bidirectional=True)\n",
            "  (output_layer): Linear(in_features=256, out_features=5, bias=True)\n",
            ")\n",
            "Input word_idxs shape: torch.Size([4, 20])\n",
            "Input valid_mask shape: torch.Size([4, 20])\n",
            "Output logits shape: torch.Size([4, 20, 5])\n"
          ]
        }
      ],
      "source": [
        "def check_lstm() -> None:\n",
        "    # Hyperparameters\n",
        "    batch_size = 4\n",
        "    d_emb = 64\n",
        "    d_hidden = 128\n",
        "    bidirectional = True\n",
        "    # Create the dataloaders and the model\n",
        "    loader_train, _, ds_train = create_sequence_dataloaders(batch_size)\n",
        "    model = LSTM(ds_train.words_vocab, ds_train.tags_vocab, d_emb, d_hidden, bidirectional)\n",
        "    device = get_device()\n",
        "    model.to(device)\n",
        "    print(model)\n",
        "    # Get the first batch\n",
        "    data_batch = next(iter(loader_train))\n",
        "    # Move data to GPU\n",
        "    word_idxs = data_batch[\"word_idxs\"].to(device, non_blocking=True)\n",
        "    tag_idxs = data_batch[\"tag_idxs\"].to(device, non_blocking=True)\n",
        "    valid_mask = data_batch[\"valid_mask\"].to(device, non_blocking=True)\n",
        "    # Calculate the model\n",
        "    print(\"Input word_idxs shape:\", word_idxs.size())\n",
        "    print(\"Input valid_mask shape:\", valid_mask.size())\n",
        "    logits = model(word_idxs, valid_mask)\n",
        "    print(\"Output logits shape:\", logits.size())\n",
        "\n",
        "\n",
        "check_lstm()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jddDYUiLY-hc"
      },
      "source": [
        "### Training and Validation **(6 points)**\n",
        "\n",
        "Complete the functions for training and validating the LSTM model. When calculating the loss function, you only want to include values from valid positions (where `valid_mask` is `True`). The `reduction` parameter in [F.cross_entropy](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.cross_entropy) may be useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hv_15mnXZ_dy"
      },
      "outputs": [],
      "source": [
        "def train_lstm(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    optimizer: optim.Optimizer,\n",
        "    device: torch.device,\n",
        "    silent: bool = False,  # whether to print the training loss\n",
        ") -> Tuple[float, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Train the LSTM model.\n",
        "    Return values:\n",
        "        1. the average training loss\n",
        "        2. training metrics such as accuracy and F1 score\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    ground_truth = []\n",
        "    predictions = []\n",
        "    losses = []\n",
        "    report_interval = 100\n",
        "\n",
        "    for i, data_batch in enumerate(loader):\n",
        "        word_idxs = data_batch[\"word_idxs\"].to(device, non_blocking=True)\n",
        "        tag_idxs = data_batch[\"tag_idxs\"].to(device, non_blocking=True)\n",
        "        valid_mask = data_batch[\"valid_mask\"].to(device, non_blocking=True)\n",
        "\n",
        "        # TODO: Do the same tasks as train_ffnn\n",
        "        # START HERE\n",
        "        # Caveat: When calculating the loss, you should only consider positions where valid_mask == True\n",
        "        # Forward pass\n",
        "        logits = model(word_idxs, valid_mask)\n",
        "\n",
        "        # Compute the loss (only consider positions where valid_mask == True)\n",
        "        loss = F.cross_entropy(logits[valid_mask], tag_idxs[valid_mask], reduction='mean')\n",
        "\n",
        "        # END\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # we get (unmasked) predictions by getting argmax of logits along last dimension (You will need to define logits!)\n",
        "        net_predictions = torch.argmax(logits, -1)\n",
        "\n",
        "        # flattening a tensor simply converts it from a multi-dimensional to a single-dimensional tensor; we flatten here to make it easier to extract ground truths and predictions\n",
        "        tag_idxs_flat = tag_idxs.flatten()\n",
        "        valid_mask_flat = valid_mask.flatten()\n",
        "        net_predictions_flat = net_predictions.flatten()\n",
        "\n",
        "        ground_truth.extend(tag_idxs_flat[valid_mask_flat].tolist())\n",
        "        predictions.extend(net_predictions_flat[valid_mask_flat].tolist())\n",
        "\n",
        "        if not silent and i > 0 and i % report_interval == 0:\n",
        "            print(\n",
        "                \"\\t[%06d/%06d] Loss: %f\"\n",
        "                % (i, len(loader), np.mean(losses[-report_interval:]))\n",
        "            )\n",
        "\n",
        "    return np.mean(losses), eval_metrics(ground_truth, predictions)\n",
        "\n",
        "\n",
        "def validate_lstm(\n",
        "    model: nn.Module, loader: DataLoader, device: torch.device\n",
        ") -> Tuple[float, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Validate the model.\n",
        "    Return the validation loss and metrics.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    ground_truth = []\n",
        "    predictions = []\n",
        "    losses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for data_batch in loader:\n",
        "            word_idxs = data_batch[\"word_idxs\"].to(device, non_blocking=True)\n",
        "            tag_idxs = data_batch[\"tag_idxs\"].to(device, non_blocking=True)\n",
        "            valid_mask = data_batch[\"valid_mask\"].to(device, non_blocking=True)\n",
        "\n",
        "            # TODO: Do the same tasks as validate_ffnn\n",
        "            # START HERE\n",
        "            # Caveat: When calculating the loss, you should only consider positions where valid_mask == True\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(word_idxs, valid_mask)\n",
        "\n",
        "            # Compute the loss (only consider positions where valid_mask == True)\n",
        "            loss = F.cross_entropy(logits[valid_mask], tag_idxs[valid_mask], reduction='mean')\n",
        "\n",
        "            # END\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            # we get (unmasked) predictions by getting argmax of logits (You will need to define logits!)\n",
        "            net_predictions = torch.argmax(logits, -1)\n",
        "\n",
        "            # flattening a tensor simply converts it from a multi-dimensional to a single-dimensional tensor; we flatten here to make it easier to extract ground truths and predictions\n",
        "            tag_idxs_flat = tag_idxs.flatten()\n",
        "            valid_mask_flat = valid_mask.flatten()\n",
        "            net_predictions_flat = net_predictions.flatten()\n",
        "\n",
        "            ground_truth.extend(tag_idxs_flat[valid_mask_flat].tolist())\n",
        "            predictions.extend(net_predictions_flat[valid_mask_flat].tolist())\n",
        "\n",
        "    return np.mean(losses), eval_metrics(ground_truth, predictions)\n",
        "\n",
        "\n",
        "def train_val_loop_lstm(hyperparams: Dict[str, Any]) -> None:\n",
        "    \"\"\"\n",
        "    Train and validate the LSTM model for a number of epochs.\n",
        "    \"\"\"\n",
        "    print(\"Hyperparameters:\", hyperparams)\n",
        "    # Create the dataloaders\n",
        "    loader_train, loader_val, ds_train = create_sequence_dataloaders(\n",
        "        hyperparams[\"batch_size\"]\n",
        "    )\n",
        "    # Create the model\n",
        "    model = LSTM(\n",
        "        ds_train.words_vocab,\n",
        "        ds_train.tags_vocab,\n",
        "        hyperparams[\"d_emb\"],\n",
        "        hyperparams[\"d_hidden\"],\n",
        "        hyperparams[\"bidirectional\"],\n",
        "    )\n",
        "    device = get_device()\n",
        "    model.to(device)\n",
        "    print(model)\n",
        "    # Create the optimizer\n",
        "    optimizer = optim.RMSprop(\n",
        "        model.parameters(), hyperparams[\"learning_rate\"], weight_decay=hyperparams[\"l2\"]\n",
        "    )\n",
        "\n",
        "    # Train and validate\n",
        "    for i in range(hyperparams[\"num_epochs\"]):\n",
        "        print(\"Epoch #%d\" % i)\n",
        "\n",
        "        print(\"Training..\")\n",
        "        loss_train, metrics_train = train_lstm(model, loader_train, optimizer, device)\n",
        "        print(\"Training loss: \", loss_train)\n",
        "        print(\"Training metrics:\")\n",
        "        for k, v in metrics_train.items():\n",
        "            print(\"\\t\", k, \": \", v)\n",
        "\n",
        "        print(\"Validating..\")\n",
        "        loss_val, metrics_val = validate_lstm(model, loader_val, device)\n",
        "        print(\"Validation loss: \", loss_val)\n",
        "        print(\"Validation metrics:\")\n",
        "        for k, v in metrics_val.items():\n",
        "            print(\"\\t\", k, \": \", v)\n",
        "\n",
        "    print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU9Nef7yal_M"
      },
      "source": [
        "Run the experiment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFxQxlokai6Z",
        "outputId": "ac25b424-1d0b-457f-dda1-2a40d09229e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameters: {'bidirectional': True, 'batch_size': 512, 'd_emb': 64, 'd_hidden': 128, 'num_epochs': 15, 'learning_rate': 0.005, 'l2': 1e-06}\n",
            "Loading data from eng.train\n",
            "14041 sentences loaded.\n",
            "Loading data from eng.val\n",
            "3490 sentences loaded.\n",
            "LSTM(\n",
            "  (word_embeddings): Embedding(20102, 64)\n",
            "  (lstm): LSTM(64, 128, batch_first=True, bidirectional=True)\n",
            "  (output_layer): Linear(in_features=256, out_features=5, bias=True)\n",
            ")\n",
            "Epoch #0\n",
            "Training..\n",
            "Training loss:  0.7228719702473393\n",
            "Training metrics:\n",
            "\t accuracy :  0.8072158617142914\n",
            "\t f1 :  [0.23237598 0.89692157 0.09400164 0.21844098 0.33414538]\n",
            "\t average f1 :  0.35517711038394173\n",
            "\t confusion matrix :  [[  1513   7178    106    647    388]\n",
            " [  1245 155293   1015   7617   1635]\n",
            " [   123   3657    286    285    168]\n",
            " [   195   7988     82   2452    252]\n",
            " [   114   5359     77    480   2126]]\n",
            "Validating..\n",
            "Validation loss:  0.30657187955720083\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9099947031740211\n",
            "\t f1 :  [0.50956807 0.96094761 0.37877671 0.62286996 0.67499321]\n",
            "\t average f1 :  0.6294311118326702\n",
            "\t confusion matrix :  [[  932   911    36   152   219]\n",
            " [  125 40847    29    93    70]\n",
            " [   74   538   257    45    93]\n",
            " [  170  1037    11  1389    83]\n",
            " [  107   517    17    91  1243]]\n",
            "Epoch #1\n",
            "Training..\n",
            "Training loss:  0.21704924051408414\n",
            "Training metrics:\n",
            "\t accuracy :  0.9341036602987632\n",
            "\t f1 :  [0.66000697 0.97512265 0.60677467 0.74117419 0.75723127]\n",
            "\t average f1 :  0.7480619519515792\n",
            "\t confusion matrix :  [[  5682   2291    314    849    710]\n",
            " [   393 165863    135    508    270]\n",
            " [   379   1244   2266    291    341]\n",
            " [   507   2382     88   7663    352]\n",
            " [   411   1240    145    375   5995]]\n",
            "Validating..\n",
            "Validation loss:  0.2034475760800498\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9368659088131035\n",
            "\t f1 :  [0.64092196 0.97625906 0.66206102 0.75321281 0.79787828]\n",
            "\t average f1 :  0.7660666263469587\n",
            "\t confusion matrix :  [[ 1585   460    59    52    94]\n",
            " [  368 40669    44    53    30]\n",
            " [  115   272   575    20    25]\n",
            " [  387   526    19  1729    29]\n",
            " [  241   225    33    47  1429]]\n",
            "Epoch #2\n",
            "Training..\n",
            "Training loss:  0.11242733288694311\n",
            "Training metrics:\n",
            "\t accuracy :  0.9661465999411245\n",
            "\t f1 :  [0.81103317 0.989653   0.78038936 0.88502255 0.86531526]\n",
            "\t average f1 :  0.8662826709474084\n",
            "\t confusion matrix :  [[  7689    925    288    517    449]\n",
            " [   312 166090    126    259    122]\n",
            " [   301    572   3287    159    205]\n",
            " [   387    712     61   9614    167]\n",
            " [   404    445    138    236   6958]]\n",
            "Validating..\n",
            "Validation loss:  0.1709140262433461\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9456871613087234\n",
            "\t f1 :  [0.67046918 0.98108621 0.72212978 0.82910652 0.82824859]\n",
            "\t average f1 :  0.8062080548714519\n",
            "\t confusion matrix :  [[ 1822   268    37    73    50]\n",
            " [  594 40356    52   140    22]\n",
            " [  146   164   651    31    15]\n",
            " [  344   195    14  2125    12]\n",
            " [  279   121    42    67  1466]]\n",
            "Epoch #3\n",
            "Training..\n",
            "Training loss:  0.06272143636036802\n",
            "Training metrics:\n",
            "\t accuracy :  0.9822662505359511\n",
            "\t f1 :  [0.89810973 0.99508357 0.87341338 0.94902426 0.92241059]\n",
            "\t average f1 :  0.9276083059763194\n",
            "\t confusion matrix :  [[  8766    443    173    244    278]\n",
            " [   191 166575     77    113     64]\n",
            " [   202    325   3819     83    108]\n",
            " [   178    235     39  10407     98]\n",
            " [   280    198    100    128   7454]]\n",
            "Validating..\n",
            "Validation loss:  0.166439627962453\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9489263741188934\n",
            "\t f1 :  [0.68289086 0.9821839  0.74322169 0.82915994 0.8643516 ]\n",
            "\t average f1 :  0.8203615966269802\n",
            "\t confusion matrix :  [[ 1852   246    43    52    57]\n",
            " [  582 40382    80    93    27]\n",
            " [  131   139   699    21    17]\n",
            " [  409   194    17  2053    17]\n",
            " [  200   104    35    43  1593]]\n",
            "Epoch #4\n",
            "Training..\n",
            "Training loss:  0.03857269289868849\n",
            "Training metrics:\n",
            "\t accuracy :  0.9896970966358477\n",
            "\t f1 :  [0.93668044 0.99729031 0.9178284  0.97550091 0.95574353]\n",
            "\t average f1 :  0.9566087206289012\n",
            "\t confusion matrix :  [[  9179    267    120    117    191]\n",
            " [   120 166725     49     49     31]\n",
            " [   145    207   4049     57     62]\n",
            " [   105     90     18  10711     41]\n",
            " [   176     93     67     61   7796]]\n",
            "Validating..\n",
            "Validation loss:  0.15887349737542017\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9535712830542313\n",
            "\t f1 :  [0.71478736 0.98298053 0.76205962 0.84694687 0.87049973]\n",
            "\t average f1 :  0.8354548216335385\n",
            "\t confusion matrix :  [[ 1832   262    39    53    64]\n",
            " [  465 40516    57   102    24]\n",
            " [  103   164   703    19    18]\n",
            " [  309   213    11  2136    21]\n",
            " [  167   116    28    44  1620]]\n",
            "Epoch #5\n",
            "Training..\n",
            "Training loss:  0.023663186157743137\n",
            "Training metrics:\n",
            "\t accuracy :  0.9940259986148687\n",
            "\t f1 :  [0.96270361 0.99833479 0.95134529 0.98930116 0.97322468]\n",
            "\t average f1 :  0.9749819066774649\n",
            "\t confusion matrix :  [[  9486    166     67     57    115]\n",
            " [    93 166968     37     19     20]\n",
            " [    77    134   4243     26     44]\n",
            " [    43     33     10  10865     19]\n",
            " [   117     55     39     28   7942]]\n",
            "Validating..\n",
            "Validation loss:  0.15558057597705297\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9574216680927352\n",
            "\t f1 :  [0.75020886 0.98332869 0.76812339 0.86371681 0.87676338]\n",
            "\t average f1 :  0.8484282260612318\n",
            "\t confusion matrix :  [[ 1796   294    47    44    69]\n",
            " [  340 40610    89    96    29]\n",
            " [   73   151   747    16    20]\n",
            " [  195   262    20  2196    17]\n",
            " [  134   116    35    43  1647]]\n",
            "Epoch #6\n",
            "Training..\n",
            "Training loss:  0.014383644969375045\n",
            "Training metrics:\n",
            "\t accuracy :  0.9967333788831647\n",
            "\t f1 :  [0.98087556 0.99898821 0.97183569 0.99512462 0.98560137]\n",
            "\t average f1 :  0.9864850913234487\n",
            "\t confusion matrix :  [[  9668    109     35     27     63]\n",
            " [    41 166862     22      9     13]\n",
            " [    46     82   4365     15     29]\n",
            " [    12     18      4  10920      7]\n",
            " [    44     44     20     15   8043]]\n",
            "Validating..\n",
            "Validation loss:  0.17605121433734894\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9539787312064539\n",
            "\t f1 :  [0.71859011 0.98329813 0.76889614 0.85295283 0.86479029]\n",
            "\t average f1 :  0.8377055002607113\n",
            "\t confusion matrix :  [[ 1896   241    27    40    46]\n",
            " [  477 40505    65   102    15]\n",
            " [  127   145   707    17    11]\n",
            " [  302   217     9  2152    10]\n",
            " [  225   114    24    45  1567]]\n",
            "Epoch #7\n",
            "Training..\n",
            "Training loss:  0.01090610199780376\n",
            "Training metrics:\n",
            "\t accuracy :  0.9973849685597365\n",
            "\t f1 :  [0.98331808 0.99924503 0.9814176  0.99681268 0.98588106]\n",
            "\t average f1 :  0.9893348887031342\n",
            "\t confusion matrix :  [[  9667     78     25     14     58]\n",
            " [    54 166768     14      6     12]\n",
            " [    30     46   4410      9     26]\n",
            " [     5      9      3  10946     12]\n",
            " [    64     33     14     12   8065]]\n",
            "Validating..\n",
            "Validation loss:  0.16466615349054337\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9593366744081816\n",
            "\t f1 :  [0.76823757 0.98381713 0.77725613 0.86597542 0.87849462]\n",
            "\t average f1 :  0.8547561748698426\n",
            "\t confusion matrix :  [[ 1785   291    50    60    64]\n",
            " [  266 40671    76   135    16]\n",
            " [   61   166   745    16    19]\n",
            " [  160   252    11  2255    12]\n",
            " [  125   136    28    52  1634]]\n",
            "Epoch #8\n",
            "Training..\n",
            "Training loss:  0.006272225451000311\n",
            "Training metrics:\n",
            "\t accuracy :  0.9989889129070497\n",
            "\t f1 :  [0.99428224 0.99964725 0.99132948 0.99849473 0.99608515]\n",
            "\t average f1 :  0.9959677715237142\n",
            "\t confusion matrix :  [[  9825     35     10      7     12]\n",
            " [    19 167200      8      5      5]\n",
            " [    11     28   4459      5      9]\n",
            " [     6      3      2  10945      4]\n",
            " [    13     15      5      1   8142]]\n",
            "Validating..\n",
            "Validation loss:  0.17822927768741334\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9578902334677912\n",
            "\t f1 :  [0.76164948 0.98330892 0.77516954 0.85952712 0.87716493]\n",
            "\t average f1 :  0.8513640000173286\n",
            "\t confusion matrix :  [[ 1847   262    36    38    67]\n",
            " [  329 40620    90    94    31]\n",
            " [   76   160   743    12    16]\n",
            " [  210   287    12  2163    18]\n",
            " [  138   126    29    36  1646]]\n",
            "Epoch #9\n",
            "Training..\n",
            "Training loss:  0.0038080331179554815\n",
            "Training metrics:\n",
            "\t accuracy :  0.9994513031550069\n",
            "\t f1 :  [0.99706002 0.99978139 0.99624309 0.99926921 0.99761482]\n",
            "\t average f1 :  0.9979937060890789\n",
            "\t confusion matrix :  [[  9835     21      4      4      6]\n",
            " [    10 166927      5      2      8]\n",
            " [     6     15   4508      1      2]\n",
            " [     3      0      0  10939      4]\n",
            " [     4     12      1      2   8156]]\n",
            "Validating..\n",
            "Validation loss:  0.18654286009924753\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9609460946094609\n",
            "\t f1 :  [0.7904272  0.98375954 0.7835703  0.86436328 0.88092729]\n",
            "\t average f1 :  0.8606095213693872\n",
            "\t confusion matrix :  [[ 1767   317    42    47    77]\n",
            " [  175 40797    75    90    27]\n",
            " [   50   179   744    12    22]\n",
            " [  130   338    10  2189    23]\n",
            " [   99   146    21    37  1672]]\n",
            "Epoch #10\n",
            "Training..\n",
            "Training loss:  0.0028043573989567382\n",
            "Training metrics:\n",
            "\t accuracy :  0.9996158777194112\n",
            "\t f1 :  [0.99878469 0.99979629 0.9967867  0.99940944 0.99877481]\n",
            "\t average f1 :  0.9987103856543753\n",
            "\t confusion matrix :  [[  9862     16      0      1      2]\n",
            " [     2 166868      3      1      6]\n",
            " [     0     23   4498      0      1]\n",
            " [     2      8      1  11000      0]\n",
            " [     1      9      1      0   8152]]\n",
            "Validating..\n",
            "Validation loss:  0.31627414907727924\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9139673226581917\n",
            "\t f1 :  [0.7630134  0.95865818 0.5237785  0.70663447 0.85817993]\n",
            "\t average f1 :  0.7620528953912975\n",
            "\t confusion matrix :  [[ 1737   123   135   180    75]\n",
            " [  355 38180  1006  1507   116]\n",
            " [   43    77   804    65    18]\n",
            " [   70    81    34  2487    18]\n",
            " [   98    28    84   110  1655]]\n",
            "Epoch #11\n",
            "Training..\n",
            "Training loss:  0.060132861240870424\n",
            "Training metrics:\n",
            "\t accuracy :  0.984069753505653\n",
            "\t f1 :  [0.91292191 0.99258476 0.93571902 0.95187756 0.96757051]\n",
            "\t average f1 :  0.9521347511005377\n",
            "\t confusion matrix :  [[  9142    357    108    166     87]\n",
            " [   798 165314    158    274    237]\n",
            " [    99    153   4236     10     31]\n",
            " [    83    444      3  10355     36]\n",
            " [    46     49     20     31   8011]]\n",
            "Validating..\n",
            "Validation loss:  0.16104757892233984\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9638389764902416\n",
            "\t f1 :  [0.82562779 0.98351912 0.79617834 0.87256673 0.89041096]\n",
            "\t average f1 :  0.8736605880423095\n",
            "\t confusion matrix :  [[ 1759   349    35    30    77]\n",
            " [   92 40938    66    50    18]\n",
            " [   25   202   750    11    19]\n",
            " [   61   434     4  2174    17]\n",
            " [   74   161    22    28  1690]]\n",
            "Epoch #12\n",
            "Training..\n",
            "Training loss:  0.005810946105392995\n",
            "Training metrics:\n",
            "\t accuracy :  0.9987278115365044\n",
            "\t f1 :  [0.99232685 0.99945185 0.99223085 0.99772417 0.99658036]\n",
            "\t average f1 :  0.9956628158599818\n",
            "\t confusion matrix :  [[  9764     56     15     14      5]\n",
            " [    42 166833     13     10     10]\n",
            " [     9     26   4470      1      3]\n",
            " [     4      9      0  10960      5]\n",
            " [     6     17      3      7   8160]]\n",
            "Validating..\n",
            "Validation loss:  0.17565372160502843\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9636759972293526\n",
            "\t f1 :  [0.82686008 0.98321395 0.80042576 0.86726384 0.89373007]\n",
            "\t average f1 :  0.8742987415663261\n",
            "\t confusion matrix :  [[ 1767   357    36    25    65]\n",
            " [   90 40972    56    33    13]\n",
            " [   24   208   752     8    15]\n",
            " [   64   478     4  2130    14]\n",
            " [   79   164    24    26  1682]]\n",
            "Epoch #13\n",
            "Training..\n",
            "Training loss:  0.003034085906283171\n",
            "Training metrics:\n",
            "\t accuracy :  0.9995805096807379\n",
            "\t f1 :  [0.9977665  0.99978411 0.99745603 0.99958947 0.99877451]\n",
            "\t average f1 :  0.998674124474987\n",
            "\t confusion matrix :  [[  9828     21      4      1      0]\n",
            " [    14 166715      3      3      6]\n",
            " [     1     13   4509      0      1]\n",
            " [     2      2      0  10957      1]\n",
            " [     1     10      1      0   8150]]\n",
            "Validating..\n",
            "Validation loss:  0.19136148797614233\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9628814733325184\n",
            "\t f1 :  [0.82622951 0.98268045 0.79808714 0.86327406 0.89102052]\n",
            "\t average f1 :  0.8722583366771554\n",
            "\t confusion matrix :  [[ 1764   364    33    25    64]\n",
            " [   91 40965    63    31    14]\n",
            " [   22   211   751     8    15]\n",
            " [   63   499     3  2112    13]\n",
            " [   80   171    25    27  1672]]\n",
            "Epoch #14\n",
            "Training..\n",
            "Training loss:  0.002043205865279392\n",
            "Training metrics:\n",
            "\t accuracy :  0.9998151118573263\n",
            "\t f1 :  [0.99893341 0.99990698 0.99878143 0.99981803 0.99957021]\n",
            "\t average f1 :  0.999402013815585\n",
            "\t confusion matrix :  [[  9834     13      0      2      0]\n",
            " [     4 166613      2      0      0]\n",
            " [     0      7   4508      0      1]\n",
            " [     2      0      0  10989      0]\n",
            " [     0      5      1      0   8140]]\n",
            "Validating..\n",
            "Validation loss:  0.20503223368099757\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9627388664792406\n",
            "\t f1 :  [0.82544939 0.9825373  0.80214477 0.86070771 0.89001332]\n",
            "\t average f1 :  0.872170497685226\n",
            "\t confusion matrix :  [[ 1745   376    33    29    67]\n",
            " [   79 40989    52    30    14]\n",
            " [   22   212   748     8    17]\n",
            " [   54   518     3  2104    11]\n",
            " [   78   176    22    28  1671]]\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "train_val_loop_lstm({\n",
        "    \"bidirectional\": True,\n",
        "    \"batch_size\": 512,\n",
        "    \"d_emb\": 64,\n",
        "    \"d_hidden\": 128,\n",
        "    \"num_epochs\": 15,\n",
        "    \"learning_rate\": 0.005,\n",
        "    \"l2\": 1e-6,\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vA-Yjqg7n0V"
      },
      "source": [
        "We were using bidirectional LSTMs. Please re-run the experiment with a regular (unidirectional) LSTM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wNrdvJ98ARB",
        "outputId": "db1ca1c6-c27f-4d7c-c2eb-bff3a08b8d33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameters: {'bidirectional': False, 'batch_size': 512, 'd_emb': 64, 'd_hidden': 128, 'num_epochs': 15, 'learning_rate': 0.005, 'l2': 1e-06}\n",
            "Loading data from eng.train\n",
            "14041 sentences loaded.\n",
            "Loading data from eng.val\n",
            "3490 sentences loaded.\n",
            "LSTM(\n",
            "  (word_embeddings): Embedding(20102, 64)\n",
            "  (lstm): LSTM(64, 128, batch_first=True)\n",
            "  (output_layer): Linear(in_features=128, out_features=5, bias=True)\n",
            ")\n",
            "Epoch #0\n",
            "Training..\n",
            "Training loss:  0.575957793880392\n",
            "Training metrics:\n",
            "\t accuracy :  0.8271015222605045\n",
            "\t f1 :  [0.09688468 0.9160156  0.06040992 0.2879605  0.38873897]\n",
            "\t average f1 :  0.3500019342092359\n",
            "\t confusion matrix :  [[   594   7104    370    802    990]\n",
            " [  1277 160098   4343    675    625]\n",
            " [   132   3050    308    362    685]\n",
            " [   217   7582    381   2216    570]\n",
            " [   182   4701    258    370   2665]]\n",
            "Validating..\n",
            "Validation loss:  0.4317032311643873\n",
            "Validation metrics:\n",
            "\t accuracy :  0.8545613820641323\n",
            "\t f1 :  [0.26896323 0.95261106 0.17077465 0.49425581 0.42920204]\n",
            "\t average f1 :  0.4631613576198294\n",
            "\t confusion matrix :  [[  406   442    17   803   582]\n",
            " [   93 38244     9  1514  1304]\n",
            " [   64   134    97   430   282]\n",
            " [   75   194     2  1936   483]\n",
            " [  131   115     4   461  1264]]\n",
            "Epoch #1\n",
            "Training..\n",
            "Training loss:  0.22858497169282702\n",
            "Training metrics:\n",
            "\t accuracy :  0.9270297123627594\n",
            "\t f1 :  [0.56175633 0.97887278 0.52437118 0.71522687 0.71117034]\n",
            "\t average f1 :  0.6982795003089309\n",
            "\t confusion matrix :  [[  5028   2087    293   1245   1238]\n",
            " [   453 165221    111    703    282]\n",
            " [   665    845   1845    475    681]\n",
            " [  1009   1858    103   7708    288]\n",
            " [   855    793    174    457   5870]]\n",
            "Validating..\n",
            "Validation loss:  0.35189234784671236\n",
            "Validation metrics:\n",
            "\t accuracy :  0.8921892189218922\n",
            "\t f1 :  [0.5325228  0.96440707 0.56787565 0.67985109 0.4985098 ]\n",
            "\t average f1 :  0.648633283082691\n",
            "\t confusion matrix :  [[ 1314   274    85    63   514]\n",
            " [  412 38882   222    44  1604]\n",
            " [  131   110   548    17   201]\n",
            " [  563   136    38  1461   492]\n",
            " [  265    68    30    23  1589]]\n",
            "Epoch #2\n",
            "Training..\n",
            "Training loss:  0.12701364661808368\n",
            "Training metrics:\n",
            "\t accuracy :  0.9605845903423758\n",
            "\t f1 :  [0.75269039 0.99056835 0.72285956 0.87661176 0.82481977]\n",
            "\t average f1 :  0.8335099631976481\n",
            "\t confusion matrix :  [[  7309    885    327    537    769]\n",
            " [   381 165836    111    323    122]\n",
            " [   532    432   2993    234    338]\n",
            " [   584    568    137   9552    116]\n",
            " [   788    336    184    190   6693]]\n",
            "Validating..\n",
            "Validation loss:  0.324652693101338\n",
            "Validation metrics:\n",
            "\t accuracy :  0.906877724809518\n",
            "\t f1 :  [0.57825033 0.9676063  0.55941656 0.77367304 0.59435364]\n",
            "\t average f1 :  0.6946599723924141\n",
            "\t confusion matrix :  [[ 1550   162    98    84   356]\n",
            " [  739 38891   497    59   978]\n",
            " [  119    76   652    22   138]\n",
            " [  437    63    31  1822   337]\n",
            " [  266    30    46    33  1600]]\n",
            "Epoch #3\n",
            "Training..\n",
            "Training loss:  0.08261083121652957\n",
            "Training metrics:\n",
            "\t accuracy :  0.9746998947699151\n",
            "\t f1 :  [0.83688087 0.99450354 0.79976663 0.93896542 0.87449293]\n",
            "\t average f1 :  0.8889218792884414\n",
            "\t confusion matrix :  [[  8237    546    289    268    559]\n",
            " [   269 166370     91    139     71]\n",
            " [   406    281   3427    142    266]\n",
            " [   238    270     92  10292     67]\n",
            " [   636    172    149    122   7114]]\n",
            "Validating..\n",
            "Validation loss:  0.3411382096154349\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9093224137228537\n",
            "\t f1 :  [0.62218515 0.96852463 0.60427807 0.78471173 0.56917632]\n",
            "\t average f1 :  0.7097751809338467\n",
            "\t confusion matrix :  [[ 1492   126    84    56   492]\n",
            " [  493 38879   408    36  1348]\n",
            " [   84    57   678    15   173]\n",
            " [  338    44    32  1817   459]\n",
            " [  139    15    35    17  1769]]\n",
            "Epoch #4\n",
            "Training..\n",
            "Training loss:  0.05801371071073744\n",
            "Training metrics:\n",
            "\t accuracy :  0.9825377998823893\n",
            "\t f1 :  [0.88085214 0.99665822 0.85346985 0.96645273 0.90698245]\n",
            "\t average f1 :  0.9208830786758689\n",
            "\t confusion matrix :  [[  8683    337    263    143    454]\n",
            " [   202 166717     57     70     52]\n",
            " [   309    188   3751     91    202]\n",
            " [   160    117     51  10616     33]\n",
            " [   481     95    127     72   7391]]\n",
            "Validating..\n",
            "Validation loss:  0.31818797971521107\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9155156256366377\n",
            "\t f1 :  [0.57668512 0.96932118 0.61423918 0.80511852 0.6932055 ]\n",
            "\t average f1 :  0.731713901334954\n",
            "\t confusion matrix :  [[ 1771   117    60    62   240]\n",
            " [ 1176 38926   374    59   629]\n",
            " [  166    57   660    17   107]\n",
            " [  528    40    19  1919   184]\n",
            " [  251    12    29    20  1663]]\n",
            "Epoch #5\n",
            "Training..\n",
            "Training loss:  0.04268957495137497\n",
            "Training metrics:\n",
            "\t accuracy :  0.9872138194475611\n",
            "\t f1 :  [0.90980114 0.9975985  0.88729834 0.9810561  0.93006436]\n",
            "\t average f1 :  0.9411636879690564\n",
            "\t confusion matrix :  [[  8967    273    221     75    323]\n",
            " [   163 166786     39     31     32]\n",
            " [   238    146   3905     55    174]\n",
            " [    95     59     28  10720     26]\n",
            " [   390     60     91     45   7587]]\n",
            "Validating..\n",
            "Validation loss:  0.3113715137754168\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9179399421423624\n",
            "\t f1 :  [0.59506734 0.96913403 0.55065913 0.83081324 0.72968852]\n",
            "\t average f1 :  0.7350724546365113\n",
            "\t confusion matrix :  [[ 1701   124   117    68   240]\n",
            " [ 1003 38918   712    68   463]\n",
            " [  125    49   731    15    87]\n",
            " [  421    48    37  2033   151]\n",
            " [  217    12    51    20  1675]]\n",
            "Epoch #6\n",
            "Training..\n",
            "Training loss:  0.03335982774970708\n",
            "Training metrics:\n",
            "\t accuracy :  0.9901016827139386\n",
            "\t f1 :  [0.92911469 0.99811255 0.91647046 0.98804199 0.94306444]\n",
            "\t average f1 :  0.9549608244908951\n",
            "\t confusion matrix :  [[  9162    208    167     51    273]\n",
            " [   150 166312     31     18     28]\n",
            " [   178    103   4087     32    129]\n",
            " [    54     43     19  10824     18]\n",
            " [   317     48     86     27   7669]]\n",
            "Validating..\n",
            "Validation loss:  0.31368871671812876\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9206494723546429\n",
            "\t f1 :  [0.55418864 0.9705458  0.65072668 0.83097145 0.79472884]\n",
            "\t average f1 :  0.7602322801496567\n",
            "\t confusion matrix :  [[ 1892   111    56    58   133]\n",
            " [ 1618 39014   304    71   157]\n",
            " [  208    51   694    14    40]\n",
            " [  525    45    24  2023    73]\n",
            " [  335    11    48    13  1568]]\n",
            "Epoch #7\n",
            "Training..\n",
            "Training loss:  0.026642235202921763\n",
            "Training metrics:\n",
            "\t accuracy :  0.99227314193989\n",
            "\t f1 :  [0.94421341 0.99851339 0.93575106 0.99186397 0.95407009]\n",
            "\t average f1 :  0.964882383962719\n",
            "\t confusion matrix :  [[  9309    170    141     25    242]\n",
            " [   108 166910     25     12     24]\n",
            " [   130     88   4180     17    100]\n",
            " [    37     30     16  10850     18]\n",
            " [   247     40     57     23   7800]]\n",
            "Validating..\n",
            "Validation loss:  0.2962318403380258\n",
            "Validation metrics:\n",
            "\t accuracy :  0.924397995355091\n",
            "\t f1 :  [0.56618771 0.97236911 0.68600509 0.8334991  0.77821394]\n",
            "\t average f1 :  0.767254989265315\n",
            "\t confusion matrix :  [[ 1852   113    45    78   162]\n",
            " [ 1472 39168   178   122   224]\n",
            " [  209    57   674    21    46]\n",
            " [  447    49    16  2095    83]\n",
            " [  312    11    45    21  1586]]\n",
            "Epoch #8\n",
            "Training..\n",
            "Training loss:  0.02221157622558099\n",
            "Training metrics:\n",
            "\t accuracy :  0.9933013876410103\n",
            "\t f1 :  [0.95008133 0.998724   0.94526809 0.99390663 0.96013208]\n",
            "\t average f1 :  0.9696224255761019\n",
            "\t confusion matrix :  [[  9345    151    132     30    212]\n",
            " [    98 166715     24     12     20]\n",
            " [   112     68   4240      8     85]\n",
            " [    28     18      9  10847     13]\n",
            " [   219     35     53     15   7851]]\n",
            "Validating..\n",
            "Validation loss:  0.32260713406971525\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9199364380882533\n",
            "\t f1 :  [0.55124816 0.96871922 0.69164882 0.84303007 0.78020658]\n",
            "\t average f1 :  0.7669705711882348\n",
            "\t confusion matrix :  [[ 1877    94    34    74   171]\n",
            " [ 1716 38850   153   159   286]\n",
            " [  233    50   646    20    58]\n",
            " [  431    40    11  2159    49]\n",
            " [  303    11    17    20  1624]]\n",
            "Epoch #9\n",
            "Training..\n",
            "Training loss:  0.017782970986984396\n",
            "Training metrics:\n",
            "\t accuracy :  0.9948107600563375\n",
            "\t f1 :  [0.96177083 0.9989504  0.9595209  0.99552471 0.9687271 ]\n",
            "\t average f1 :  0.9768987864686972\n",
            "\t confusion matrix :  [[  9472    132     96     15    156]\n",
            " [    82 166555     19      6     22]\n",
            " [    81     39   4326      9     68]\n",
            " [    19     23      6  10900      8]\n",
            " [   172     27     47     12   7930]]\n",
            "Validating..\n",
            "Validation loss:  0.28746357560157776\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9280854011327059\n",
            "\t f1 :  [0.61481612 0.97292873 0.67770204 0.84151743 0.73864871]\n",
            "\t average f1 :  0.7691226058927828\n",
            "\t confusion matrix :  [[ 1747   119    60    74   250]\n",
            " [ 1002 39228   230   204   500]\n",
            " [  141    60   696    19    91]\n",
            " [  340    54    24  2185    87]\n",
            " [  203    14    37    21  1700]]\n",
            "Epoch #10\n",
            "Training..\n",
            "Training loss:  0.014540306671901985\n",
            "Training metrics:\n",
            "\t accuracy :  0.9956477086906234\n",
            "\t f1 :  [0.96765048 0.99915587 0.96712994 0.99635369 0.9726388 ]\n",
            "\t average f1 :  0.980585755469155\n",
            "\t confusion matrix :  [[  9557    101     80     19    139]\n",
            " [    66 166895     21      5     15]\n",
            " [    54     42   4384      5     55]\n",
            " [    17     10      6  10930      9]\n",
            " [   163     22     35      9   7945]]\n",
            "Validating..\n",
            "Validation loss:  0.26076023067746845\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9333007374811555\n",
            "\t f1 :  [0.62373961 0.9760657  0.67168391 0.84288577 0.76823038]\n",
            "\t average f1 :  0.77652107352081\n",
            "\t confusion matrix :  [[ 1763   156    68    59   204]\n",
            " [  923 39578   263   106   294]\n",
            " [  140    75   714    13    65]\n",
            " [  350    99    24  2103   114]\n",
            " [  227    25    50    19  1654]]\n",
            "Epoch #11\n",
            "Training..\n",
            "Training loss:  0.015385352833955377\n",
            "Training metrics:\n",
            "\t accuracy :  0.9952368615102537\n",
            "\t f1 :  [0.96341773 0.99920742 0.96576775 0.99644323 0.9671419 ]\n",
            "\t average f1 :  0.9783956032103743\n",
            "\t confusion matrix :  [[  9494    100     87     13    179]\n",
            " [    59 167042     18      5     21]\n",
            " [    59     35   4387     11     58]\n",
            " [    16     10      6  10926     12]\n",
            " [   208     17     37      5   7903]]\n",
            "Validating..\n",
            "Validation loss:  0.23332693108490535\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9389235219818278\n",
            "\t f1 :  [0.6422255  0.9777603  0.70444104 0.85421896 0.78436019]\n",
            "\t average f1 :  0.7926011984572119\n",
            "\t confusion matrix :  [[ 1743   188    53    63   203]\n",
            " [  788 39788   164   167   257]\n",
            " [  141    93   690    18    65]\n",
            " [  288   113    12  2212    65]\n",
            " [  218    40    33    29  1655]]\n",
            "Epoch #12\n",
            "Training..\n",
            "Training loss:  0.010987808013817778\n",
            "Training metrics:\n",
            "\t accuracy :  0.9966606935175526\n",
            "\t f1 :  [0.97393425 0.99939465 0.97727525 0.9976247  0.9776802 ]\n",
            "\t average f1 :  0.9851818107345874\n",
            "\t confusion matrix :  [[  9584     75     59      9    132]\n",
            " [    47 166744     13      2     19]\n",
            " [    44     25   4408      3     33]\n",
            " [    16      8      2  10920      5]\n",
            " [   131     13     26      7   8016]]\n",
            "Validating..\n",
            "Validation loss:  0.2419705433504922\n",
            "Validation metrics:\n",
            "\t accuracy :  0.936784419182659\n",
            "\t f1 :  [0.64464352 0.97686717 0.64888889 0.846213   0.79476414]\n",
            "\t average f1 :  0.7822753428997826\n",
            "\t confusion matrix :  [[ 1727   180   102    71   170]\n",
            " [  738 39716   319   191   200]\n",
            " [  127    88   730    19    43]\n",
            " [  290   124    23  2201    52]\n",
            " [  226    41    69    30  1609]]\n",
            "Epoch #13\n",
            "Training..\n",
            "Training loss:  0.009949314666705\n",
            "Training metrics:\n",
            "\t accuracy :  0.9968151117456482\n",
            "\t f1 :  [0.97437722 0.99946946 0.97787121 0.99744735 0.97929518]\n",
            "\t average f1 :  0.9856920856801136\n",
            "\t confusion matrix :  [[  9583     68     65     11    121]\n",
            " [    44 166723     16      5     15]\n",
            " [    45     18   4419      3     28]\n",
            " [    17      4      3  10941      6]\n",
            " [   133      7     22      7   8017]]\n",
            "Validating..\n",
            "Validation loss:  0.22795374478612626\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9458705129772237\n",
            "\t f1 :  [0.67175888 0.97885715 0.72598253 0.83459725 0.82896764]\n",
            "\t average f1 :  0.8080326911079367\n",
            "\t confusion matrix :  [[ 1627   366    45    48   164]\n",
            " [  448 40487    86    75    68]\n",
            " [  110   175   665    10    47]\n",
            " [  211   410     7  2036    26]\n",
            " [  198   121    22    20  1614]]\n",
            "Epoch #14\n",
            "Training..\n",
            "Training loss:  0.04524825133935169\n",
            "Training metrics:\n",
            "\t accuracy :  0.9885806934397605\n",
            "\t f1 :  [0.94669792 0.99507784 0.93328915 0.97055058 0.96086584]\n",
            "\t average f1 :  0.9612962677185614\n",
            "\t confusion matrix :  [[  9289    308     90     20    160]\n",
            " [   222 166178    175    258    103]\n",
            " [    56    180   4225      8     56]\n",
            " [    28    297      7  10612     10]\n",
            " [   162    101     32     16   7857]]\n",
            "Validating..\n",
            "Validation loss:  0.1921020703656333\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9465020576131687\n",
            "\t f1 :  [0.67665867 0.98087008 0.7321238  0.85682493 0.80576594]\n",
            "\t average f1 :  0.8104486831017501\n",
            "\t confusion matrix :  [[ 1693   222    45    97   193]\n",
            " [  540 40122    97   219   186]\n",
            " [  124   114   686    34    49]\n",
            " [  197   131    11  2310    41]\n",
            " [  200    56    28    42  1649]]\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "## TODO: Re-run with unidirectional LSTMs\n",
        "## Keep other hyperparameters fixed\n",
        "train_val_loop_lstm({\n",
        "    \"bidirectional\": False,\n",
        "    \"batch_size\": 512,\n",
        "    \"d_emb\": 64,\n",
        "    \"d_hidden\": 128,\n",
        "    \"num_epochs\": 15,\n",
        "    \"learning_rate\": 0.005,\n",
        "    \"l2\": 1e-6,\n",
        "})\n",
        "## END"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8UChDyKaPBs"
      },
      "source": [
        "### Questions **(2 points)**\n",
        "\n",
        "(a) How does the final performance of LSTMs compare to FFNNs? Is it better? What is a possible explanation?\n",
        "\n",
        "**TODO: Please fill in your answer here**\n",
        "\n",
        "FFNN final performance outclassed uni-directional LSTMs by a fine margin in both accuracy and average f1, while both were beaten by the bi-directional LSTM, Overall, the performance differences observed between the FFNN, uni-directional LSTM, and bi-directional LSTM in the context of named entity recognition can be attributed to their respective abilities to capture contextual information, handle long-range dependencies, and effectively learn from the available data.\n",
        "\n",
        "(b) How does bidirectional LSTMs compare to unidirectional LSTMs? Why?\n",
        "\n",
        "**TODO: Please fill in your answer here**\n",
        "\n",
        " It's evident that the bi-directional LSTM outperformed the uni-directional LSTM in terms of both training and validation metrics. Here's a comparison:\n",
        "\n",
        "      Training Loss and Accuracy: The bi-directional LSTM achieved a significantly lower training loss and higher training accuracy compared to the uni-directional LSTM. This indicates that the bi-directional LSTM learned the training data better and more efficiently, resulting in a more accurate model.\n",
        "\n",
        "      Validation Performance: The bi-directional LSTM also demonstrated superior performance on the validation dataset, with higher accuracy and average F1 score across all classes. This suggests that the bi-directional LSTM generalized better to unseen data, indicating robustness and effectiveness in capturing patterns beyond the training set.\n",
        "\n",
        "      F1 Scores: Specifically, the bi-directional LSTM achieved higher F1 scores for most classes compared to the uni-directional LSTM, indicating better precision and recall. This is particularly important for tasks like named entity recognition, where achieving a balance between precision and recall is crucial.\n",
        "\n",
        "      Confusion Matrix: The confusion matrices show that the bi-directional LSTM made fewer misclassifications across different classes compared to the uni-directional LSTM, further corroborating its superior performance.\n",
        "\n",
        "The superior performance of the bi-directional LSTM over the uni-directional LSTM can be attributed to its ability to leverage information from both past and future tokens in the input sequence. This bidirectional context modeling allows the bi-directional LSTM to capture richer contextual information, which is especially beneficial for tasks like named entity recognition where context plays a crucial role. Additionally, the bi-directional LSTM's architecture enables it to capture long-range dependencies more effectively, contributing to its superior performance in capturing complex patterns within the text data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMEWxkN_bpIT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}